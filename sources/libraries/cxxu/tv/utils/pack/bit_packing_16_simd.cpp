//
// Automatically generated by gen-bit-packers, do not edit
//
#include <immintrin.h>

#include <cxxu/pack/bit_packing_16.hpp>

namespace cxxu {
namespace pack {

//
// Various SIMD intrinsics helpers to ease writing
//
inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
adapt_mask_16(const uint32_t b)
{ return _mm_set1_epi16((uint16_t) b); }

inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
adapt_mask_16(const __m128i b)
{ return b; }

template <typename T>
inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
band_16(__m128i a, const T b)
{ return _mm_and_si128(a, adapt_mask_16(b)); }

template <typename T>
inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
band_set_16(__m128i a, const T b)
{ a = _mm_and_si128(a, adapt_mask_16(b)); return a; }

template <typename T>
inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
bor_16(__m128i a, const T b)
{ return _mm_or_si128(a, adapt_mask_16(b)); }

template <typename T>
inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
bor_set_16(__m128i a, const T b)
{ a = _mm_or_si128(a, adapt_mask_16(b)); return a; }

inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
shl_16(__m128i a, int count)
{ return _mm_slli_epi16(a, count); }

inline
__m128i __attribute__((__gnu_inline__, __always_inline__))
shr_16(__m128i a, int count)
{ return _mm_srli_epi16(a, count); }

void
pack_16_into_1_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 1, w += 16) {
        out[c + 0]  = shl_16(band_16(in[w +  0], 0x0001U),  0);
        out[c + 0] |= shl_16(band_16(in[w +  1], 0x0001U),  1);
        out[c + 0] |= shl_16(band_16(in[w +  2], 0x0001U),  2);
        out[c + 0] |= shl_16(band_16(in[w +  3], 0x0001U),  3);
        out[c + 0] |= shl_16(band_16(in[w +  4], 0x0001U),  4);
        out[c + 0] |= shl_16(band_16(in[w +  5], 0x0001U),  5);
        out[c + 0] |= shl_16(band_16(in[w +  6], 0x0001U),  6);
        out[c + 0] |= shl_16(band_16(in[w +  7], 0x0001U),  7);
        out[c + 0] |= shl_16(band_16(in[w +  8], 0x0001U),  8);
        out[c + 0] |= shl_16(band_16(in[w +  9], 0x0001U),  9);
        out[c + 0] |= shl_16(band_16(in[w + 10], 0x0001U), 10);
        out[c + 0] |= shl_16(band_16(in[w + 11], 0x0001U), 11);
        out[c + 0] |= shl_16(band_16(in[w + 12], 0x0001U), 12);
        out[c + 0] |= shl_16(band_16(in[w + 13], 0x0001U), 13);
        out[c + 0] |= shl_16(band_16(in[w + 14], 0x0001U), 14);
        out[c + 0] |= shl_16(band_16(in[w + 15], 0x0001U), 15);
    }
}

void
pack_16_into_2_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 1, w += 8) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x0003U),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x0003U),  2);
        out[c + 0] |= shl_16(band_16(in[w + 2], 0x0003U),  4);
        out[c + 0] |= shl_16(band_16(in[w + 3], 0x0003U),  6);
        out[c + 0] |= shl_16(band_16(in[w + 4], 0x0003U),  8);
        out[c + 0] |= shl_16(band_16(in[w + 5], 0x0003U), 10);
        out[c + 0] |= shl_16(band_16(in[w + 6], 0x0003U), 12);
        out[c + 0] |= shl_16(band_16(in[w + 7], 0x0003U), 14);
    }
}

void
pack_16_into_3_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 3, w += 16) {
        out[c + 0]  = shl_16(band_16(in[w +  0], 0x0007U),  0);
        out[c + 0] |= shl_16(band_16(in[w +  1], 0x0007U),  3);
        out[c + 0] |= shl_16(band_16(in[w +  2], 0x0007U),  6);
        out[c + 0] |= shl_16(band_16(in[w +  3], 0x0007U),  9);
        out[c + 0] |= shl_16(band_16(in[w +  4], 0x0007U), 12);
        out[c + 0] |= shl_16(band_16(in[w +  5], 0x0007U), 15);
        out[c + 1]  = bor_16(shl_16(band_16(in[w +  6], 0x0007U),  2), shr_16(band_16(in[w +  5], 0x0007U), 1));
        out[c + 1] |= shl_16(band_16(in[w +  7], 0x0007U),  5);
        out[c + 1] |= shl_16(band_16(in[w +  8], 0x0007U),  8);
        out[c + 1] |= shl_16(band_16(in[w +  9], 0x0007U), 11);
        out[c + 1] |= shl_16(band_16(in[w + 10], 0x0007U), 14);
        out[c + 2]  = bor_16(shl_16(band_16(in[w + 11], 0x0007U),  1), shr_16(band_16(in[w + 10], 0x0007U), 2));
        out[c + 2] |= shl_16(band_16(in[w + 12], 0x0007U),  4);
        out[c + 2] |= shl_16(band_16(in[w + 13], 0x0007U),  7);
        out[c + 2] |= shl_16(band_16(in[w + 14], 0x0007U), 10);
        out[c + 2] |= shl_16(band_16(in[w + 15], 0x0007U), 13);
    }
}

void
pack_16_into_4_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 1, w += 4) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x000fU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x000fU),  4);
        out[c + 0] |= shl_16(band_16(in[w + 2], 0x000fU),  8);
        out[c + 0] |= shl_16(band_16(in[w + 3], 0x000fU), 12);
    }
}

void
pack_16_into_5_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 5, w += 16) {
        out[c + 0]  = shl_16(band_16(in[w +  0], 0x001fU),  0);
        out[c + 0] |= shl_16(band_16(in[w +  1], 0x001fU),  5);
        out[c + 0] |= shl_16(band_16(in[w +  2], 0x001fU), 10);
        out[c + 0] |= shl_16(band_16(in[w +  3], 0x001fU), 15);
        out[c + 1]  = bor_16(shl_16(band_16(in[w +  4], 0x001fU),  4), shr_16(band_16(in[w +  3], 0x001fU), 1));
        out[c + 1] |= shl_16(band_16(in[w +  5], 0x001fU),  9);
        out[c + 1] |= shl_16(band_16(in[w +  6], 0x001fU), 14);
        out[c + 2]  = bor_16(shl_16(band_16(in[w +  7], 0x001fU),  3), shr_16(band_16(in[w +  6], 0x001fU), 2));
        out[c + 2] |= shl_16(band_16(in[w +  8], 0x001fU),  8);
        out[c + 2] |= shl_16(band_16(in[w +  9], 0x001fU), 13);
        out[c + 3]  = bor_16(shl_16(band_16(in[w + 10], 0x001fU),  2), shr_16(band_16(in[w +  9], 0x001fU), 3));
        out[c + 3] |= shl_16(band_16(in[w + 11], 0x001fU),  7);
        out[c + 3] |= shl_16(band_16(in[w + 12], 0x001fU), 12);
        out[c + 4]  = bor_16(shl_16(band_16(in[w + 13], 0x001fU),  1), shr_16(band_16(in[w + 12], 0x001fU), 4));
        out[c + 4] |= shl_16(band_16(in[w + 14], 0x001fU),  6);
        out[c + 4] |= shl_16(band_16(in[w + 15], 0x001fU), 11);
    }
}

void
pack_16_into_6_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 3, w += 8) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x003fU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x003fU),  6);
        out[c + 0] |= shl_16(band_16(in[w + 2], 0x003fU), 12);
        out[c + 1]  = bor_16(shl_16(band_16(in[w + 3], 0x003fU),  2), shr_16(band_16(in[w + 2], 0x003fU), 4));
        out[c + 1] |= shl_16(band_16(in[w + 4], 0x003fU),  8);
        out[c + 1] |= shl_16(band_16(in[w + 5], 0x003fU), 14);
        out[c + 2]  = bor_16(shl_16(band_16(in[w + 6], 0x003fU),  4), shr_16(band_16(in[w + 5], 0x003fU), 2));
        out[c + 2] |= shl_16(band_16(in[w + 7], 0x003fU), 10);
    }
}

void
pack_16_into_7_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 7, w += 16) {
        out[c + 0]  = shl_16(band_16(in[w +  0], 0x007fU),  0);
        out[c + 0] |= shl_16(band_16(in[w +  1], 0x007fU),  7);
        out[c + 0] |= shl_16(band_16(in[w +  2], 0x007fU), 14);
        out[c + 1]  = bor_16(shl_16(band_16(in[w +  3], 0x007fU),  5), shr_16(band_16(in[w +  2], 0x007fU), 2));
        out[c + 1] |= shl_16(band_16(in[w +  4], 0x007fU), 12);
        out[c + 2]  = bor_16(shl_16(band_16(in[w +  5], 0x007fU),  3), shr_16(band_16(in[w +  4], 0x007fU), 4));
        out[c + 2] |= shl_16(band_16(in[w +  6], 0x007fU), 10);
        out[c + 3]  = bor_16(shl_16(band_16(in[w +  7], 0x007fU),  1), shr_16(band_16(in[w +  6], 0x007fU), 6));
        out[c + 3] |= shl_16(band_16(in[w +  8], 0x007fU),  8);
        out[c + 3] |= shl_16(band_16(in[w +  9], 0x007fU), 15);
        out[c + 4]  = bor_16(shl_16(band_16(in[w + 10], 0x007fU),  6), shr_16(band_16(in[w +  9], 0x007fU), 1));
        out[c + 4] |= shl_16(band_16(in[w + 11], 0x007fU), 13);
        out[c + 5]  = bor_16(shl_16(band_16(in[w + 12], 0x007fU),  4), shr_16(band_16(in[w + 11], 0x007fU), 3));
        out[c + 5] |= shl_16(band_16(in[w + 13], 0x007fU), 11);
        out[c + 6]  = bor_16(shl_16(band_16(in[w + 14], 0x007fU),  2), shr_16(band_16(in[w + 13], 0x007fU), 5));
        out[c + 6] |= shl_16(band_16(in[w + 15], 0x007fU),  9);
    }
}

void
pack_16_into_8_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 1, w += 2) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x00ffU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x00ffU),  8);
    }
}

void
pack_16_into_9_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 9, w += 16) {
        out[c + 0]  = shl_16(band_16(in[w +  0], 0x01ffU),  0);
        out[c + 0] |= shl_16(band_16(in[w +  1], 0x01ffU),  9);
        out[c + 1]  = bor_16(shl_16(band_16(in[w +  2], 0x01ffU),  2), shr_16(band_16(in[w +  1], 0x01ffU), 7));
        out[c + 1] |= shl_16(band_16(in[w +  3], 0x01ffU), 11);
        out[c + 2]  = bor_16(shl_16(band_16(in[w +  4], 0x01ffU),  4), shr_16(band_16(in[w +  3], 0x01ffU), 5));
        out[c + 2] |= shl_16(band_16(in[w +  5], 0x01ffU), 13);
        out[c + 3]  = bor_16(shl_16(band_16(in[w +  6], 0x01ffU),  6), shr_16(band_16(in[w +  5], 0x01ffU), 3));
        out[c + 3] |= shl_16(band_16(in[w +  7], 0x01ffU), 15);
        out[c + 4]  = bor_16(shl_16(band_16(in[w +  8], 0x01ffU),  8), shr_16(band_16(in[w +  7], 0x01ffU), 1));
        out[c + 5]  = bor_16(shl_16(band_16(in[w +  9], 0x01ffU),  1), shr_16(band_16(in[w +  8], 0x01ffU), 8));
        out[c + 5] |= shl_16(band_16(in[w + 10], 0x01ffU), 10);
        out[c + 6]  = bor_16(shl_16(band_16(in[w + 11], 0x01ffU),  3), shr_16(band_16(in[w + 10], 0x01ffU), 6));
        out[c + 6] |= shl_16(band_16(in[w + 12], 0x01ffU), 12);
        out[c + 7]  = bor_16(shl_16(band_16(in[w + 13], 0x01ffU),  5), shr_16(band_16(in[w + 12], 0x01ffU), 4));
        out[c + 7] |= shl_16(band_16(in[w + 14], 0x01ffU), 14);
        out[c + 8]  = bor_16(shl_16(band_16(in[w + 15], 0x01ffU),  7), shr_16(band_16(in[w + 14], 0x01ffU), 2));
    }
}

void
pack_16_into_10_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 5, w += 8) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x03ffU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x03ffU), 10);
        out[c + 1]  = bor_16(shl_16(band_16(in[w + 2], 0x03ffU),  4), shr_16(band_16(in[w + 1], 0x03ffU), 6));
        out[c + 1] |= shl_16(band_16(in[w + 3], 0x03ffU), 14);
        out[c + 2]  = bor_16(shl_16(band_16(in[w + 4], 0x03ffU),  8), shr_16(band_16(in[w + 3], 0x03ffU), 2));
        out[c + 3]  = bor_16(shl_16(band_16(in[w + 5], 0x03ffU),  2), shr_16(band_16(in[w + 4], 0x03ffU), 8));
        out[c + 3] |= shl_16(band_16(in[w + 6], 0x03ffU), 12);
        out[c + 4]  = bor_16(shl_16(band_16(in[w + 7], 0x03ffU),  6), shr_16(band_16(in[w + 6], 0x03ffU), 4));
    }
}

void
pack_16_into_11_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 11, w += 16) {
        out[c +  0]  = shl_16(band_16(in[w +  0], 0x07ffU),  0);
        out[c +  0] |= shl_16(band_16(in[w +  1], 0x07ffU), 11);
        out[c +  1]  = bor_16(shl_16(band_16(in[w +  2], 0x07ffU),  6), shr_16(band_16(in[w +  1], 0x07ffU), 5));
        out[c +  2]  = bor_16(shl_16(band_16(in[w +  3], 0x07ffU),  1), shr_16(band_16(in[w +  2], 0x07ffU), 10));
        out[c +  2] |= shl_16(band_16(in[w +  4], 0x07ffU), 12);
        out[c +  3]  = bor_16(shl_16(band_16(in[w +  5], 0x07ffU),  7), shr_16(band_16(in[w +  4], 0x07ffU), 4));
        out[c +  4]  = bor_16(shl_16(band_16(in[w +  6], 0x07ffU),  2), shr_16(band_16(in[w +  5], 0x07ffU), 9));
        out[c +  4] |= shl_16(band_16(in[w +  7], 0x07ffU), 13);
        out[c +  5]  = bor_16(shl_16(band_16(in[w +  8], 0x07ffU),  8), shr_16(band_16(in[w +  7], 0x07ffU), 3));
        out[c +  6]  = bor_16(shl_16(band_16(in[w +  9], 0x07ffU),  3), shr_16(band_16(in[w +  8], 0x07ffU), 8));
        out[c +  6] |= shl_16(band_16(in[w + 10], 0x07ffU), 14);
        out[c +  7]  = bor_16(shl_16(band_16(in[w + 11], 0x07ffU),  9), shr_16(band_16(in[w + 10], 0x07ffU), 2));
        out[c +  8]  = bor_16(shl_16(band_16(in[w + 12], 0x07ffU),  4), shr_16(band_16(in[w + 11], 0x07ffU), 7));
        out[c +  8] |= shl_16(band_16(in[w + 13], 0x07ffU), 15);
        out[c +  9]  = bor_16(shl_16(band_16(in[w + 14], 0x07ffU), 10), shr_16(band_16(in[w + 13], 0x07ffU), 1));
        out[c + 10]  = bor_16(shl_16(band_16(in[w + 15], 0x07ffU),  5), shr_16(band_16(in[w + 14], 0x07ffU), 6));
    }
}

void
pack_16_into_12_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 3, w += 4) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x0fffU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x0fffU), 12);
        out[c + 1]  = bor_16(shl_16(band_16(in[w + 2], 0x0fffU),  8), shr_16(band_16(in[w + 1], 0x0fffU), 4));
        out[c + 2]  = bor_16(shl_16(band_16(in[w + 3], 0x0fffU),  4), shr_16(band_16(in[w + 2], 0x0fffU), 8));
    }
}

void
pack_16_into_13_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 13, w += 16) {
        out[c +  0]  = shl_16(band_16(in[w +  0], 0x1fffU),  0);
        out[c +  0] |= shl_16(band_16(in[w +  1], 0x1fffU), 13);
        out[c +  1]  = bor_16(shl_16(band_16(in[w +  2], 0x1fffU), 10), shr_16(band_16(in[w +  1], 0x1fffU), 3));
        out[c +  2]  = bor_16(shl_16(band_16(in[w +  3], 0x1fffU),  7), shr_16(band_16(in[w +  2], 0x1fffU), 6));
        out[c +  3]  = bor_16(shl_16(band_16(in[w +  4], 0x1fffU),  4), shr_16(band_16(in[w +  3], 0x1fffU), 9));
        out[c +  4]  = bor_16(shl_16(band_16(in[w +  5], 0x1fffU),  1), shr_16(band_16(in[w +  4], 0x1fffU), 12));
        out[c +  4] |= shl_16(band_16(in[w +  6], 0x1fffU), 14);
        out[c +  5]  = bor_16(shl_16(band_16(in[w +  7], 0x1fffU), 11), shr_16(band_16(in[w +  6], 0x1fffU), 2));
        out[c +  6]  = bor_16(shl_16(band_16(in[w +  8], 0x1fffU),  8), shr_16(band_16(in[w +  7], 0x1fffU), 5));
        out[c +  7]  = bor_16(shl_16(band_16(in[w +  9], 0x1fffU),  5), shr_16(band_16(in[w +  8], 0x1fffU), 8));
        out[c +  8]  = bor_16(shl_16(band_16(in[w + 10], 0x1fffU),  2), shr_16(band_16(in[w +  9], 0x1fffU), 11));
        out[c +  8] |= shl_16(band_16(in[w + 11], 0x1fffU), 15);
        out[c +  9]  = bor_16(shl_16(band_16(in[w + 12], 0x1fffU), 12), shr_16(band_16(in[w + 11], 0x1fffU), 1));
        out[c + 10]  = bor_16(shl_16(band_16(in[w + 13], 0x1fffU),  9), shr_16(band_16(in[w + 12], 0x1fffU), 4));
        out[c + 11]  = bor_16(shl_16(band_16(in[w + 14], 0x1fffU),  6), shr_16(band_16(in[w + 13], 0x1fffU), 7));
        out[c + 12]  = bor_16(shl_16(band_16(in[w + 15], 0x1fffU),  3), shr_16(band_16(in[w + 14], 0x1fffU), 10));
    }
}

void
pack_16_into_14_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 7, w += 8) {
        out[c + 0]  = shl_16(band_16(in[w + 0], 0x3fffU),  0);
        out[c + 0] |= shl_16(band_16(in[w + 1], 0x3fffU), 14);
        out[c + 1]  = bor_16(shl_16(band_16(in[w + 2], 0x3fffU), 12), shr_16(band_16(in[w + 1], 0x3fffU), 2));
        out[c + 2]  = bor_16(shl_16(band_16(in[w + 3], 0x3fffU), 10), shr_16(band_16(in[w + 2], 0x3fffU), 4));
        out[c + 3]  = bor_16(shl_16(band_16(in[w + 4], 0x3fffU),  8), shr_16(band_16(in[w + 3], 0x3fffU), 6));
        out[c + 4]  = bor_16(shl_16(band_16(in[w + 5], 0x3fffU),  6), shr_16(band_16(in[w + 4], 0x3fffU), 8));
        out[c + 5]  = bor_16(shl_16(band_16(in[w + 6], 0x3fffU),  4), shr_16(band_16(in[w + 5], 0x3fffU), 10));
        out[c + 6]  = bor_16(shl_16(band_16(in[w + 7], 0x3fffU),  2), shr_16(band_16(in[w + 6], 0x3fffU), 12));
    }
}

void
pack_16_into_15_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t c = 0, w = 0; w < n; c += 15, w += 16) {
        out[c +  0]  = shl_16(band_16(in[w +  0], 0x7fffU),  0);
        out[c +  0] |= shl_16(band_16(in[w +  1], 0x7fffU), 15);
        out[c +  1]  = bor_16(shl_16(band_16(in[w +  2], 0x7fffU), 14), shr_16(band_16(in[w +  1], 0x7fffU), 1));
        out[c +  2]  = bor_16(shl_16(band_16(in[w +  3], 0x7fffU), 13), shr_16(band_16(in[w +  2], 0x7fffU), 2));
        out[c +  3]  = bor_16(shl_16(band_16(in[w +  4], 0x7fffU), 12), shr_16(band_16(in[w +  3], 0x7fffU), 3));
        out[c +  4]  = bor_16(shl_16(band_16(in[w +  5], 0x7fffU), 11), shr_16(band_16(in[w +  4], 0x7fffU), 4));
        out[c +  5]  = bor_16(shl_16(band_16(in[w +  6], 0x7fffU), 10), shr_16(band_16(in[w +  5], 0x7fffU), 5));
        out[c +  6]  = bor_16(shl_16(band_16(in[w +  7], 0x7fffU),  9), shr_16(band_16(in[w +  6], 0x7fffU), 6));
        out[c +  7]  = bor_16(shl_16(band_16(in[w +  8], 0x7fffU),  8), shr_16(band_16(in[w +  7], 0x7fffU), 7));
        out[c +  8]  = bor_16(shl_16(band_16(in[w +  9], 0x7fffU),  7), shr_16(band_16(in[w +  8], 0x7fffU), 8));
        out[c +  9]  = bor_16(shl_16(band_16(in[w + 10], 0x7fffU),  6), shr_16(band_16(in[w +  9], 0x7fffU), 9));
        out[c + 10]  = bor_16(shl_16(band_16(in[w + 11], 0x7fffU),  5), shr_16(band_16(in[w + 10], 0x7fffU), 10));
        out[c + 11]  = bor_16(shl_16(band_16(in[w + 12], 0x7fffU),  4), shr_16(band_16(in[w + 11], 0x7fffU), 11));
        out[c + 12]  = bor_16(shl_16(band_16(in[w + 13], 0x7fffU),  3), shr_16(band_16(in[w + 12], 0x7fffU), 12));
        out[c + 13]  = bor_16(shl_16(band_16(in[w + 14], 0x7fffU),  2), shr_16(band_16(in[w + 13], 0x7fffU), 13));
        out[c + 14]  = bor_16(shl_16(band_16(in[w + 15], 0x7fffU),  1), shr_16(band_16(in[w + 14], 0x7fffU), 14));
    }
}
void
unpack_1_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 1) {
        out[w +  0] = shr_16(band_16(in[c + 0], 0x0001U),  0);
        out[w +  1] = shr_16(band_16(in[c + 0], 0x0002U),  1);
        out[w +  2] = shr_16(band_16(in[c + 0], 0x0004U),  2);
        out[w +  3] = shr_16(band_16(in[c + 0], 0x0008U),  3);
        out[w +  4] = shr_16(band_16(in[c + 0], 0x0010U),  4);
        out[w +  5] = shr_16(band_16(in[c + 0], 0x0020U),  5);
        out[w +  6] = shr_16(band_16(in[c + 0], 0x0040U),  6);
        out[w +  7] = shr_16(band_16(in[c + 0], 0x0080U),  7);
        out[w +  8] = shr_16(band_16(in[c + 0], 0x0100U),  8);
        out[w +  9] = shr_16(band_16(in[c + 0], 0x0200U),  9);
        out[w + 10] = shr_16(band_16(in[c + 0], 0x0400U), 10);
        out[w + 11] = shr_16(band_16(in[c + 0], 0x0800U), 11);
        out[w + 12] = shr_16(band_16(in[c + 0], 0x1000U), 12);
        out[w + 13] = shr_16(band_16(in[c + 0], 0x2000U), 13);
        out[w + 14] = shr_16(band_16(in[c + 0], 0x4000U), 14);
        out[w + 15] = shr_16(band_16(in[c + 0], 0x8000U), 15);
    }
}
void
unpack_2_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 8, c += 1) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x0003U),  0);
        out[w + 1] = shr_16(band_16(in[c + 0], 0x000cU),  2);
        out[w + 2] = shr_16(band_16(in[c + 0], 0x0030U),  4);
        out[w + 3] = shr_16(band_16(in[c + 0], 0x00c0U),  6);
        out[w + 4] = shr_16(band_16(in[c + 0], 0x0300U),  8);
        out[w + 5] = shr_16(band_16(in[c + 0], 0x0c00U), 10);
        out[w + 6] = shr_16(band_16(in[c + 0], 0x3000U), 12);
        out[w + 7] = shr_16(band_16(in[c + 0], 0xc000U), 14);
    }
}
void
unpack_3_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 3) {
        out[w +  0] = shr_16(band_16(in[c + 0], 0x0007U),  0);
        out[w +  1] = shr_16(band_16(in[c + 0], 0x0038U),  3);
        out[w +  2] = shr_16(band_16(in[c + 0], 0x01c0U),  6);
        out[w +  3] = shr_16(band_16(in[c + 0], 0x0e00U),  9);
        out[w +  4] = shr_16(band_16(in[c + 0], 0x7000U), 12);
        out[w +  5] = bor_16(shr_16(band_16(in[c + 0], 0x8000U), 15), shl_16(band_16(in[c + 1], 0x0003U), 1));
        out[w +  6] = shr_16(band_16(in[c + 1], 0x001cU),  2);
        out[w +  7] = shr_16(band_16(in[c + 1], 0x00e0U),  5);
        out[w +  8] = shr_16(band_16(in[c + 1], 0x0700U),  8);
        out[w +  9] = shr_16(band_16(in[c + 1], 0x3800U), 11);
        out[w + 10] = bor_16(shr_16(band_16(in[c + 1], 0xc000U), 14), shl_16(band_16(in[c + 2], 0x0001U), 2));
        out[w + 11] = shr_16(band_16(in[c + 2], 0x000eU),  1);
        out[w + 12] = shr_16(band_16(in[c + 2], 0x0070U),  4);
        out[w + 13] = shr_16(band_16(in[c + 2], 0x0380U),  7);
        out[w + 14] = shr_16(band_16(in[c + 2], 0x1c00U), 10);
        out[w + 15] = shr_16(band_16(in[c + 2], 0xe000U), 13);
    }
}
void
unpack_4_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 4, c += 1) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x000fU),  0);
        out[w + 1] = shr_16(band_16(in[c + 0], 0x00f0U),  4);
        out[w + 2] = shr_16(band_16(in[c + 0], 0x0f00U),  8);
        out[w + 3] = shr_16(band_16(in[c + 0], 0xf000U), 12);
    }
}
void
unpack_5_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 5) {
        out[w +  0] = shr_16(band_16(in[c + 0], 0x001fU),  0);
        out[w +  1] = shr_16(band_16(in[c + 0], 0x03e0U),  5);
        out[w +  2] = shr_16(band_16(in[c + 0], 0x7c00U), 10);
        out[w +  3] = bor_16(shr_16(band_16(in[c + 0], 0x8000U), 15), shl_16(band_16(in[c + 1], 0x000fU), 1));
        out[w +  4] = shr_16(band_16(in[c + 1], 0x01f0U),  4);
        out[w +  5] = shr_16(band_16(in[c + 1], 0x3e00U),  9);
        out[w +  6] = bor_16(shr_16(band_16(in[c + 1], 0xc000U), 14), shl_16(band_16(in[c + 2], 0x0007U), 2));
        out[w +  7] = shr_16(band_16(in[c + 2], 0x00f8U),  3);
        out[w +  8] = shr_16(band_16(in[c + 2], 0x1f00U),  8);
        out[w +  9] = bor_16(shr_16(band_16(in[c + 2], 0xe000U), 13), shl_16(band_16(in[c + 3], 0x0003U), 3));
        out[w + 10] = shr_16(band_16(in[c + 3], 0x007cU),  2);
        out[w + 11] = shr_16(band_16(in[c + 3], 0x0f80U),  7);
        out[w + 12] = bor_16(shr_16(band_16(in[c + 3], 0xf000U), 12), shl_16(band_16(in[c + 4], 0x0001U), 4));
        out[w + 13] = shr_16(band_16(in[c + 4], 0x003eU),  1);
        out[w + 14] = shr_16(band_16(in[c + 4], 0x07c0U),  6);
        out[w + 15] = shr_16(band_16(in[c + 4], 0xf800U), 11);
    }
}
void
unpack_6_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 8, c += 3) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x003fU),  0);
        out[w + 1] = shr_16(band_16(in[c + 0], 0x0fc0U),  6);
        out[w + 2] = bor_16(shr_16(band_16(in[c + 0], 0xf000U), 12), shl_16(band_16(in[c + 1], 0x0003U), 4));
        out[w + 3] = shr_16(band_16(in[c + 1], 0x00fcU),  2);
        out[w + 4] = shr_16(band_16(in[c + 1], 0x3f00U),  8);
        out[w + 5] = bor_16(shr_16(band_16(in[c + 1], 0xc000U), 14), shl_16(band_16(in[c + 2], 0x000fU), 2));
        out[w + 6] = shr_16(band_16(in[c + 2], 0x03f0U),  4);
        out[w + 7] = shr_16(band_16(in[c + 2], 0xfc00U), 10);
    }
}
void
unpack_7_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 7) {
        out[w +  0] = shr_16(band_16(in[c + 0], 0x007fU),  0);
        out[w +  1] = shr_16(band_16(in[c + 0], 0x3f80U),  7);
        out[w +  2] = bor_16(shr_16(band_16(in[c + 0], 0xc000U), 14), shl_16(band_16(in[c + 1], 0x001fU), 2));
        out[w +  3] = shr_16(band_16(in[c + 1], 0x0fe0U),  5);
        out[w +  4] = bor_16(shr_16(band_16(in[c + 1], 0xf000U), 12), shl_16(band_16(in[c + 2], 0x0007U), 4));
        out[w +  5] = shr_16(band_16(in[c + 2], 0x03f8U),  3);
        out[w +  6] = bor_16(shr_16(band_16(in[c + 2], 0xfc00U), 10), shl_16(band_16(in[c + 3], 0x0001U), 6));
        out[w +  7] = shr_16(band_16(in[c + 3], 0x00feU),  1);
        out[w +  8] = shr_16(band_16(in[c + 3], 0x7f00U),  8);
        out[w +  9] = bor_16(shr_16(band_16(in[c + 3], 0x8000U), 15), shl_16(band_16(in[c + 4], 0x003fU), 1));
        out[w + 10] = shr_16(band_16(in[c + 4], 0x1fc0U),  6);
        out[w + 11] = bor_16(shr_16(band_16(in[c + 4], 0xe000U), 13), shl_16(band_16(in[c + 5], 0x000fU), 3));
        out[w + 12] = shr_16(band_16(in[c + 5], 0x07f0U),  4);
        out[w + 13] = bor_16(shr_16(band_16(in[c + 5], 0xf800U), 11), shl_16(band_16(in[c + 6], 0x0003U), 5));
        out[w + 14] = shr_16(band_16(in[c + 6], 0x01fcU),  2);
        out[w + 15] = shr_16(band_16(in[c + 6], 0xfe00U),  9);
    }
}
void
unpack_8_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 2, c += 1) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x00ffU),  0);
        out[w + 1] = shr_16(band_16(in[c + 0], 0xff00U),  8);
    }
}
void
unpack_9_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 9) {
        out[w +  0] = shr_16(band_16(in[c + 0], 0x01ffU),  0);
        out[w +  1] = bor_16(shr_16(band_16(in[c + 0], 0xfe00U),  9), shl_16(band_16(in[c + 1], 0x0003U), 7));
        out[w +  2] = shr_16(band_16(in[c + 1], 0x07fcU),  2);
        out[w +  3] = bor_16(shr_16(band_16(in[c + 1], 0xf800U), 11), shl_16(band_16(in[c + 2], 0x000fU), 5));
        out[w +  4] = shr_16(band_16(in[c + 2], 0x1ff0U),  4);
        out[w +  5] = bor_16(shr_16(band_16(in[c + 2], 0xe000U), 13), shl_16(band_16(in[c + 3], 0x003fU), 3));
        out[w +  6] = shr_16(band_16(in[c + 3], 0x7fc0U),  6);
        out[w +  7] = bor_16(shr_16(band_16(in[c + 3], 0x8000U), 15), shl_16(band_16(in[c + 4], 0x00ffU), 1));
        out[w +  8] = bor_16(shr_16(band_16(in[c + 4], 0xff00U),  8), shl_16(band_16(in[c + 5], 0x0001U), 8));
        out[w +  9] = shr_16(band_16(in[c + 5], 0x03feU),  1);
        out[w + 10] = bor_16(shr_16(band_16(in[c + 5], 0xfc00U), 10), shl_16(band_16(in[c + 6], 0x0007U), 6));
        out[w + 11] = shr_16(band_16(in[c + 6], 0x0ff8U),  3);
        out[w + 12] = bor_16(shr_16(band_16(in[c + 6], 0xf000U), 12), shl_16(band_16(in[c + 7], 0x001fU), 4));
        out[w + 13] = shr_16(band_16(in[c + 7], 0x3fe0U),  5);
        out[w + 14] = bor_16(shr_16(band_16(in[c + 7], 0xc000U), 14), shl_16(band_16(in[c + 8], 0x007fU), 2));
        out[w + 15] = shr_16(band_16(in[c + 8], 0xff80U),  7);
    }
}
void
unpack_10_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 8, c += 5) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x03ffU),  0);
        out[w + 1] = bor_16(shr_16(band_16(in[c + 0], 0xfc00U), 10), shl_16(band_16(in[c + 1], 0x000fU), 6));
        out[w + 2] = shr_16(band_16(in[c + 1], 0x3ff0U),  4);
        out[w + 3] = bor_16(shr_16(band_16(in[c + 1], 0xc000U), 14), shl_16(band_16(in[c + 2], 0x00ffU), 2));
        out[w + 4] = bor_16(shr_16(band_16(in[c + 2], 0xff00U),  8), shl_16(band_16(in[c + 3], 0x0003U), 8));
        out[w + 5] = shr_16(band_16(in[c + 3], 0x0ffcU),  2);
        out[w + 6] = bor_16(shr_16(band_16(in[c + 3], 0xf000U), 12), shl_16(band_16(in[c + 4], 0x003fU), 4));
        out[w + 7] = shr_16(band_16(in[c + 4], 0xffc0U),  6);
    }
}
void
unpack_11_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 11) {
        out[w +  0] = shr_16(band_16(in[c +  0], 0x07ffU),  0);
        out[w +  1] = bor_16(shr_16(band_16(in[c +  0], 0xf800U), 11), shl_16(band_16(in[c +  1], 0x003fU), 5));
        out[w +  2] = bor_16(shr_16(band_16(in[c +  1], 0xffc0U),  6), shl_16(band_16(in[c +  2], 0x0001U), 10));
        out[w +  3] = shr_16(band_16(in[c +  2], 0x0ffeU),  1);
        out[w +  4] = bor_16(shr_16(band_16(in[c +  2], 0xf000U), 12), shl_16(band_16(in[c +  3], 0x007fU), 4));
        out[w +  5] = bor_16(shr_16(band_16(in[c +  3], 0xff80U),  7), shl_16(band_16(in[c +  4], 0x0003U), 9));
        out[w +  6] = shr_16(band_16(in[c +  4], 0x1ffcU),  2);
        out[w +  7] = bor_16(shr_16(band_16(in[c +  4], 0xe000U), 13), shl_16(band_16(in[c +  5], 0x00ffU), 3));
        out[w +  8] = bor_16(shr_16(band_16(in[c +  5], 0xff00U),  8), shl_16(band_16(in[c +  6], 0x0007U), 8));
        out[w +  9] = shr_16(band_16(in[c +  6], 0x3ff8U),  3);
        out[w + 10] = bor_16(shr_16(band_16(in[c +  6], 0xc000U), 14), shl_16(band_16(in[c +  7], 0x01ffU), 2));
        out[w + 11] = bor_16(shr_16(band_16(in[c +  7], 0xfe00U),  9), shl_16(band_16(in[c +  8], 0x000fU), 7));
        out[w + 12] = shr_16(band_16(in[c +  8], 0x7ff0U),  4);
        out[w + 13] = bor_16(shr_16(band_16(in[c +  8], 0x8000U), 15), shl_16(band_16(in[c +  9], 0x03ffU), 1));
        out[w + 14] = bor_16(shr_16(band_16(in[c +  9], 0xfc00U), 10), shl_16(band_16(in[c + 10], 0x001fU), 6));
        out[w + 15] = shr_16(band_16(in[c + 10], 0xffe0U),  5);
    }
}
void
unpack_12_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 4, c += 3) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x0fffU),  0);
        out[w + 1] = bor_16(shr_16(band_16(in[c + 0], 0xf000U), 12), shl_16(band_16(in[c + 1], 0x00ffU), 4));
        out[w + 2] = bor_16(shr_16(band_16(in[c + 1], 0xff00U),  8), shl_16(band_16(in[c + 2], 0x000fU), 8));
        out[w + 3] = shr_16(band_16(in[c + 2], 0xfff0U),  4);
    }
}
void
unpack_13_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 13) {
        out[w +  0] = shr_16(band_16(in[c +  0], 0x1fffU),  0);
        out[w +  1] = bor_16(shr_16(band_16(in[c +  0], 0xe000U), 13), shl_16(band_16(in[c +  1], 0x03ffU), 3));
        out[w +  2] = bor_16(shr_16(band_16(in[c +  1], 0xfc00U), 10), shl_16(band_16(in[c +  2], 0x007fU), 6));
        out[w +  3] = bor_16(shr_16(band_16(in[c +  2], 0xff80U),  7), shl_16(band_16(in[c +  3], 0x000fU), 9));
        out[w +  4] = bor_16(shr_16(band_16(in[c +  3], 0xfff0U),  4), shl_16(band_16(in[c +  4], 0x0001U), 12));
        out[w +  5] = shr_16(band_16(in[c +  4], 0x3ffeU),  1);
        out[w +  6] = bor_16(shr_16(band_16(in[c +  4], 0xc000U), 14), shl_16(band_16(in[c +  5], 0x07ffU), 2));
        out[w +  7] = bor_16(shr_16(band_16(in[c +  5], 0xf800U), 11), shl_16(band_16(in[c +  6], 0x00ffU), 5));
        out[w +  8] = bor_16(shr_16(band_16(in[c +  6], 0xff00U),  8), shl_16(band_16(in[c +  7], 0x001fU), 8));
        out[w +  9] = bor_16(shr_16(band_16(in[c +  7], 0xffe0U),  5), shl_16(band_16(in[c +  8], 0x0003U), 11));
        out[w + 10] = shr_16(band_16(in[c +  8], 0x7ffcU),  2);
        out[w + 11] = bor_16(shr_16(band_16(in[c +  8], 0x8000U), 15), shl_16(band_16(in[c +  9], 0x0fffU), 1));
        out[w + 12] = bor_16(shr_16(band_16(in[c +  9], 0xf000U), 12), shl_16(band_16(in[c + 10], 0x01ffU), 4));
        out[w + 13] = bor_16(shr_16(band_16(in[c + 10], 0xfe00U),  9), shl_16(band_16(in[c + 11], 0x003fU), 7));
        out[w + 14] = bor_16(shr_16(band_16(in[c + 11], 0xffc0U),  6), shl_16(band_16(in[c + 12], 0x0007U), 10));
        out[w + 15] = shr_16(band_16(in[c + 12], 0xfff8U),  3);
    }
}
void
unpack_14_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 8, c += 7) {
        out[w + 0] = shr_16(band_16(in[c + 0], 0x3fffU),  0);
        out[w + 1] = bor_16(shr_16(band_16(in[c + 0], 0xc000U), 14), shl_16(band_16(in[c + 1], 0x0fffU), 2));
        out[w + 2] = bor_16(shr_16(band_16(in[c + 1], 0xf000U), 12), shl_16(band_16(in[c + 2], 0x03ffU), 4));
        out[w + 3] = bor_16(shr_16(band_16(in[c + 2], 0xfc00U), 10), shl_16(band_16(in[c + 3], 0x00ffU), 6));
        out[w + 4] = bor_16(shr_16(band_16(in[c + 3], 0xff00U),  8), shl_16(band_16(in[c + 4], 0x003fU), 8));
        out[w + 5] = bor_16(shr_16(band_16(in[c + 4], 0xffc0U),  6), shl_16(band_16(in[c + 5], 0x000fU), 10));
        out[w + 6] = bor_16(shr_16(band_16(in[c + 5], 0xfff0U),  4), shl_16(band_16(in[c + 6], 0x0003U), 12));
        out[w + 7] = shr_16(band_16(in[c + 6], 0xfffcU),  2);
    }
}
void
unpack_15_into_16_simd(
    uint16_t* __restrict__ out_ptr,
    const uint16_t* __restrict__ in_ptr,
    uint32_t n
)
{
    __m128i* out = reinterpret_cast<__m128i*>(out_ptr);
    const __m128i* in = reinterpret_cast<const __m128i*>(in_ptr);

    for (uint32_t w = 0, c = 0; w < n; w += 16, c += 15) {
        out[w +  0] = shr_16(band_16(in[c +  0], 0x7fffU),  0);
        out[w +  1] = bor_16(shr_16(band_16(in[c +  0], 0x8000U), 15), shl_16(band_16(in[c +  1], 0x3fffU), 1));
        out[w +  2] = bor_16(shr_16(band_16(in[c +  1], 0xc000U), 14), shl_16(band_16(in[c +  2], 0x1fffU), 2));
        out[w +  3] = bor_16(shr_16(band_16(in[c +  2], 0xe000U), 13), shl_16(band_16(in[c +  3], 0x0fffU), 3));
        out[w +  4] = bor_16(shr_16(band_16(in[c +  3], 0xf000U), 12), shl_16(band_16(in[c +  4], 0x07ffU), 4));
        out[w +  5] = bor_16(shr_16(band_16(in[c +  4], 0xf800U), 11), shl_16(band_16(in[c +  5], 0x03ffU), 5));
        out[w +  6] = bor_16(shr_16(band_16(in[c +  5], 0xfc00U), 10), shl_16(band_16(in[c +  6], 0x01ffU), 6));
        out[w +  7] = bor_16(shr_16(band_16(in[c +  6], 0xfe00U),  9), shl_16(band_16(in[c +  7], 0x00ffU), 7));
        out[w +  8] = bor_16(shr_16(band_16(in[c +  7], 0xff00U),  8), shl_16(band_16(in[c +  8], 0x007fU), 8));
        out[w +  9] = bor_16(shr_16(band_16(in[c +  8], 0xff80U),  7), shl_16(band_16(in[c +  9], 0x003fU), 9));
        out[w + 10] = bor_16(shr_16(band_16(in[c +  9], 0xffc0U),  6), shl_16(band_16(in[c + 10], 0x001fU), 10));
        out[w + 11] = bor_16(shr_16(band_16(in[c + 10], 0xffe0U),  5), shl_16(band_16(in[c + 11], 0x000fU), 11));
        out[w + 12] = bor_16(shr_16(band_16(in[c + 11], 0xfff0U),  4), shl_16(band_16(in[c + 12], 0x0007U), 12));
        out[w + 13] = bor_16(shr_16(band_16(in[c + 12], 0xfff8U),  3), shl_16(band_16(in[c + 13], 0x0003U), 13));
        out[w + 14] = bor_16(shr_16(band_16(in[c + 13], 0xfffcU),  2), shl_16(band_16(in[c + 14], 0x0001U), 14));
        out[w + 15] = shr_16(band_16(in[c + 14], 0xfffeU),  1);
    }
}

packer_16_ptr packer_16_simd_table[16] = {
    0,
    pack_16_into_1_simd,
    pack_16_into_2_simd,
    pack_16_into_3_simd,
    pack_16_into_4_simd,
    pack_16_into_5_simd,
    pack_16_into_6_simd,
    pack_16_into_7_simd,
    pack_16_into_8_simd,
    pack_16_into_9_simd,
    pack_16_into_10_simd,
    pack_16_into_11_simd,
    pack_16_into_12_simd,
    pack_16_into_13_simd,
    pack_16_into_14_simd,
    pack_16_into_15_simd
};

unpacker_16_ptr unpacker_16_simd_table[16] = {
    0,
    unpack_1_into_16_simd,
    unpack_2_into_16_simd,
    unpack_3_into_16_simd,
    unpack_4_into_16_simd,
    unpack_5_into_16_simd,
    unpack_6_into_16_simd,
    unpack_7_into_16_simd,
    unpack_8_into_16_simd,
    unpack_9_into_16_simd,
    unpack_10_into_16_simd,
    unpack_11_into_16_simd,
    unpack_12_into_16_simd,
    unpack_13_into_16_simd,
    unpack_14_into_16_simd,
    unpack_15_into_16_simd
};

} // namespace pack
} // namespace cxxu
